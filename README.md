# RAINBOW LUNAR LANDER ğŸš€

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)

A modular, high-performance reinforcement learning experimentation platform featuring Real-Time Visualization for the Lunar Lander environment. Built with **PyTorch**, **PyQt6**, and **Gymnasium**, optimized for CUDA acceleration.

**Keywords:** Reinforcement Learning, RL, Lunar Lander, Rainbow DQN, Double DQN, PyTorch, CUDA, PyQt6, Real-time Visualization, Neural Network Visualization, Gymnasium, AI, Artificial Intelligence, Deep Learning.

---

## ğŸ¥ Demo

<div align="center">
  <video src="https://github.com/NandeeshaHK/RL-Sim/raw/refs/heads/main/assets/public/sample_video.mp4" 
         autoplay 
         loop 
         muted 
         playsinline 
         width="100%">
  </video>
</div>

---

## ğŸ·ï¸ GitHub Topics (Manual Setup)

Since GitHub topics cannot be set via git commands, please **manually add these topics** to the repository's "About" settings (top-right of repo page):

`reinforcement-learning` `rainbow-dqn` `pytorch` `cuda-optimized` `real-time-visualization` `gymnasium` `lunar-lander` `deep-learning` `ai` `python` `pyqt6` `gpu-acceleration` `custom-metrics`

---

## âœ¨ Features

- **Advanced RL Algorithms**: 
  - **Double DQN**: Deep Q-Network with double Q-learning and epsilon-greedy exploration.
  - **Rainbow DQN**: State-of-the-art integration of C51, PER, Dueling Nets, Noisy Nets, and N-step returns.
- **Real-time Visualization**: 
  - ğŸ–¥ï¸ **Live Rendering**: Watch the agent learn in real-time.
  - ğŸ§  **Neural Network Viz**: See activations and Q-values propagate through the network.
  - ğŸ“ˆ **Live Metrics**: Monitor Rewards, Loss, Q-values, and V-values with dynamic graphs.
- **Interactive Control Panel**: 
  - Play/Pause/Stop training.
  - Variable speed control (1x to Max).
  - Hot-swappable algorithms.
- **High Performance**: 
  - Multi-instance support (train multiple agents).
  - CUDA optimization for 4GB VRAM budgets.

## ğŸ› ï¸ Installation

```bash
# Clone the repository
git clone https://github.com/NandeeshaHK/RL-Sim.git
cd RL-Sim

# Install with UV (Recommended for speed & CUDA support)
uv sync

# Verify CUDA installation
uv run check.torch.py
```

## ğŸš€ Quick Start

```bash
# Launch the GUI
uv run python main.py

# Launch with specific algorithm
uv run python main.py --algorithm rainbow
```

## ğŸ“‚ Project Structure

```
rainbow_lunarlander/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ algorithms/      # RL Agents (DQN, Rainbow)
â”‚   â”œâ”€â”€ buffers/         # Replay Buffers (Uniform, PER)
â”‚   â”œâ”€â”€ environments/    # Gym Wrappers
â”‚   â”œâ”€â”€ gui/             # PyQt6 Panels & Widgets
â”‚   â”œâ”€â”€ core/            # Training Loop & Metrics
â”‚   â””â”€â”€ utils/           # CUDA & Helper utils
â”œâ”€â”€ config/              # Hyperparameters (YAML)
â”œâ”€â”€ assets/              # Images, Videos, Styles
â””â”€â”€ main.py              # Application Entry Point
```

## âš™ï¸ Configuration

Customize your experiments in `config/default.yaml`:
- **Hyperparameters**: Learning rate, gamma, batch size.
- **Network**: Hidden layer sizes, atom counts (C51).
- **Environment**: Reward shaping, stacking.

## ğŸ¤ Contributing

Contributions are welcome! Please fork the repository and submit a pull request.

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.
